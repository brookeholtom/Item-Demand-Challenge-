summary(augustine_lm_best)
predictions_tree_augustine
collect_metrics(CV_results_tree_augustine) %>%
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
collect_metrics(CV_results_tree_augustine) %>%
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
avPlots(augustine_lm_avg, Average= ~ Rooms + Baths + ProfessionalPhotography)
autoplot(augustine_lm_avg, which = 2, ncol = 1, nrow = 1) +
theme(aspect.ratio = 1)
collect_metrics(CV_results_tree_augustine) %>%
filter(.metric=="rmse") #%>%
rsq(predictions_tree_augustine)
rsq(truth = augustine$Occupancy, estimate = predictions_tree_augustine)
collect_metrics(CV_results_tree_augustine) %>%
filter(.metric=="rmse") #%>%
final_wf_tree
my_recipe_augustine <- recipe(Occupancy*Average ~ ., data=augustine)
my_recipe_augustine <- recipe(Average ~ ., data=augustine)
prepped_recipe_augustine <- prep(my_recipe_augustine)
bake(prepped_recipe_augustine, new_data=augustine)
my_mod_tree_augustine <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use
set_mode("regression")
## Create a workflow with model & recipe
preg_wf_tree_augustine <- workflow() %>%
add_recipe(my_recipe_augustine) %>%
add_model(my_mod_tree_augustine)
## Set up grid of tuning values
tuning_grid_tree_augustine <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 5)
## Set up K-fold CV
folds <- vfold_cv(augustine, v =5, repeats=1)
## Find best tuning parameters
CV_results_tree_augustine <- preg_wf_tree_augustine %>%
tune_grid(resamples=folds,
grid=tuning_grid_tree_augustine,
metrics=metric_set(rmse, mae, rsq))
bestTune_augustine <- CV_results_tree_augustine %>%
select_best("rmse")
final_wf_tree <- preg_wf_tree_augustine %>%
finalize_workflow(bestTune_augustine) %>%
fit(data=augustine)
predictions_tree_augustine <- final_wf_tree %>%
predict(new_data = augustine)
predictions_tree_augustine
collect_metrics(CV_results_tree_augustine) %>%
filter(.metric=="rmse") #%>%
final_wf_tree
augustine_tree <- augustine %>%
mutate(OccAvg = Occupancy * Average) %>%
select(-High, -Low)
my_recipe_augustine <- recipe(OccAvg ~ ., data=augustine)
augustine_tree <- augustine %>%
mutate(OccAvg = Occupancy * Average) %>%
select(-High, -Low)
my_recipe_augustine <- recipe(OccAvg ~ ., data=augustine)
augustine_tree
my_recipe_augustine <- recipe(OccAvg ~ ., data=augustine_tree)
prepped_recipe_augustine <- prep(my_recipe_augustine)
bake(prepped_recipe_augustine, new_data=augustine_tree)
my_mod_tree_augustine <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use
set_mode("regression")
## Create a workflow with model & recipe
preg_wf_tree_augustine <- workflow() %>%
add_recipe(my_recipe_augustine) %>%
add_model(my_mod_tree_augustine)
## Set up grid of tuning values
tuning_grid_tree_augustine <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 5)
## Set up K-fold CV
folds <- vfold_cv(augustine_tree, v =5, repeats=1)
## Find best tuning parameters
CV_results_tree_augustine <- preg_wf_tree_augustine %>%
tune_grid(resamples=folds,
grid=tuning_grid_tree_augustine,
metrics=metric_set(rmse, mae, rsq))
bestTune_augustine <- CV_results_tree_augustine %>%
select_best("rmse")
final_wf_tree <- preg_wf_tree_augustine %>%
finalize_workflow(bestTune_augustine) %>%
fit(data=augustine_tree)
predictions_tree_augustine <- final_wf_tree %>%
predict(new_data = augustine_tree)
predictions_tree_augustine
collect_metrics(CV_results_tree_augustine) %>%
filter(.metric=="rmse") #%>%
final_wf_tree
augustine_tree <- augustine %>%
mutate(OccAvg = Occupancy * Average) %>%
select(-High, -Low, -Occupancy, -Average)
augustine_tree
my_recipe_augustine <- recipe(OccAvg ~ ., data=augustine_tree)
prepped_recipe_augustine <- prep(my_recipe_augustine)
bake(prepped_recipe_augustine, new_data=augustine_tree)
my_mod_tree_augustine <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use
set_mode("regression")
## Create a workflow with model & recipe
preg_wf_tree_augustine <- workflow() %>%
add_recipe(my_recipe_augustine) %>%
add_model(my_mod_tree_augustine)
## Set up grid of tuning values
tuning_grid_tree_augustine <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 5)
## Set up K-fold CV
folds <- vfold_cv(augustine_tree, v =5, repeats=1)
## Find best tuning parameters
CV_results_tree_augustine <- preg_wf_tree_augustine %>%
tune_grid(resamples=folds,
grid=tuning_grid_tree_augustine,
metrics=metric_set(rmse, mae, rsq))
bestTune_augustine <- CV_results_tree_augustine %>%
select_best("rmse")
final_wf_tree <- preg_wf_tree_augustine %>%
finalize_workflow(bestTune_augustine) %>%
fit(data=augustine_tree)
predictions_tree_augustine <- final_wf_tree %>%
predict(new_data = augustine_tree)
predictions_tree_augustine
collect_metrics(CV_results_tree_augustine) %>%
filter(.metric=="rmse") #%>%
final_wf_tree
RMSE = sqrt(nanmean((OccAvg_predicted-OccAvg).^2))
1 - mse / Var(OccAvg)
mse(predictions_tree_augustine)
performance_mse(predictions_tree_augustine)
summary(final_wf_tree)
final_wf_tree
performance::performance_mse(predictions_tree_augustine)
library(performance)
performance::mse(predictions_tree_augustine)
performance::mse(final_wf_tree)
CV_results_tree_augustine
bestTune_augustine
predictions_tree_augustine
(predictions_tree_augustine - augustine_tree)
predictions_tree_augustine
augustine_tree
(predictions_tree_augustine - augustine_tree$OccAvg)
(predictions_tree_augustine - augustine_tree$OccAvg)
(predictions_tree_augustine - augustine_tree$OccAvg)^2/75
sum(predictions_tree_augustine - augustine_tree$OccAvg)
sum(predictions_tree_augustine - augustine_tree$OccAvg)^2
sum((predictions_tree_augustine - augustine_tree$OccAvg)^2)
sum((predictions_tree_augustine - augustine_tree$OccAvg)^2)/75
LR_R = RSQUARE(augustine_tree$OccAvg,predictions_tree_augustine)
RSQUARE = function(y_actual,y_predict){
cor(y_actual,y_predict)^2
}
LR_R = RSQUARE(augustine_tree$OccAvg,predictions_tree_augustine)
LR_R
RSqaured_Augustine
RSQUARE = function(y_actual,y_predict){
cor(y_actual,y_predict)^2
}
RSquared_Augustine = RSQUARE(augustine_tree$OccAvg,predictions_tree_augustine)
RSquared_Augustine
knitr::opts_chunk$set(echo = TRUE)
###Loading Packages###
library(tidyverse)
library(corrplot)
library(bestglm)
library(car)
library(vroom)
library(tidymodels)
view(augustine)
###Importing Data and Creating a "Recipe"###
augustine <- vroom("StAugustine.csv") %>%
select(-PL, -PH)
view(augustine)
mutate(3mAvg = (DaysBooked3Months/92)*Average %>%
mutate(ThreeMAvg = (DaysBooked3Months/92)*Average %>%
augustine <- augustine %>%
###Importing Data and Creating a "Recipe"###
augustine <- vroom("StAugustine.csv") %>%
select(-PL, -PH)
###Importing Data and Creating a "Recipe"###
augustine <- vroom("StAugustine.csv") %>%
select(-PL, -PH)
augustine <- augustine %>%
###EDA###
augustine <- augustine %>%
mutate(BookedAvg = (DaysBookedinNext30/30)*Average) %>%
mutate(ThreeMAvg = (DaysBooked3Months/92)*Average %>%
select(-KidFriendly, -ClimateControl, -FastWifi, -KitchenEssentials, -'W&D')
ggplot(data = augustine) +
mutate(ThreeAvg = (DaysBooked3Months/92)*Average %>%
library(tidyverse)
library(ggplot2)
library(vroom)
library(tidymodels)
library(embed)
library(ranger)
library(discrim)
library(naivebayes)
library(kknn)
library(themis)
#Imputation
train_missing <- read.csv("C:/Users/brook/Downloads/STAT348/GhostsGhoulsandGoblins/trainWithMissingValues.csv")
train <- read.csv("C:/Users/brook/Downloads/STAT348/GhostsGhoulsandGoblins/train.csv")
test <- read.csv("C:/Users/brook/Downloads/STAT348/GhostsGhoulsandGoblins/test.csv")
#Naive Bayes Final Model
nb_recipe <- recipe(type ~., data=train) %>%
update_role(id, new_role="id") %>%
step_lencode_glm(all_nominal_predictors(), outcome=vars(type)) %>%
step_interact(~ hair_length + bone_length) %>%
step_normalize(all_numeric_predictors()) %>%
step_range(all_numeric_predictors(), min=0, max=1)  #scale to [0,1]
nb_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes") # install discrim library for the naivebayes eng6
nb_wf <- workflow() %>%
add_recipe(nb_recipe) %>%
add_model(nb_model)
## Tune smoothness and Laplace here
tuning_grid_nb <- grid_regular(Laplace(),
smoothness(),
levels = 5) ## L^2 total tuning possibilities
## Set up K-fold CV
folds <- vfold_cv(train, v = 5, repeats=1)
CV_results_nb <- nb_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid_nb,
metrics=metric_set(accuracy))
## Find best tuning parameters
bestTune_nb <- CV_results_nb %>%
select_best("accuracy")
final_wf_nb <-
nb_wf %>%
finalize_workflow(bestTune_nb) %>%
fit(data=train)
predictions_nb <- final_wf_nb %>%
predict(test, type = "class")
predictions_nb <- predictions_nb %>%
bind_cols(., test) %>%
select(id, .pred_class) %>%
rename(type = .pred_class)
vroom_write(x= predictions_nb, file="predictions_nb_2.csv", delim=",")
predictions_nb <- predictions_nb %>%
bind_cols(., test) %>%
select(id, .pred_class) %>%
rename(type = .pred_class)
#Naive Bayes Final Model
nb_recipe <- recipe(type ~., data=train) %>%
update_role(id, new_role="id") %>%
step_lencode_glm(all_nominal_predictors(), outcome=vars(type)) %>%
step_interact(~ hair_length + bone_length) %>%
step_normalize(all_numeric_predictors()) %>%
step_range(all_numeric_predictors(), min=0, max=1)  #scale to [0,1]
nb_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes") # install discrim library for the naivebayes eng6
nb_wf <- workflow() %>%
add_recipe(nb_recipe) %>%
add_model(nb_model)
## Tune smoothness and Laplace here
tuning_grid_nb <- grid_regular(Laplace(),
smoothness(),
levels = 5) ## L^2 total tuning possibilities
## Set up K-fold CV
folds <- vfold_cv(train, v = 5, repeats=1)
CV_results_nb <- nb_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid_nb,
metrics=metric_set(accuracy))
## Find best tuning parameters
bestTune_nb <- CV_results_nb %>%
select_best("accuracy")
final_wf_nb <-
nb_wf %>%
finalize_workflow(bestTune_nb) %>%
fit(data=train)
predictions_nb <- final_wf_nb %>%
predict(test, type = "class")
predictions_nb
predictions_nb <- predictions_nb %>%
bind_cols(., test) %>%
select(id, .pred_class) %>%
rename(type = .pred_class)
vroom_write(x= predictions_nb, file="predictions_nb_2.csv", delim=",")
predictions_nb <- predictions_nb %>%
bind_cols(., test) %>%
select(id, .pred_class) %>%
rename(type = .pred_class)
#Naive Bayes Final Model
nb_recipe <- recipe(type ~., data=train) %>%
#update_role(id, new_role="id") %>%
step_lencode_glm(all_nominal_predictors(), outcome=vars(type)) %>%
step_interact(~ hair_length + bone_length) %>%
step_normalize(all_numeric_predictors()) %>%
step_range(all_numeric_predictors(), min=0, max=1)  #scale to [0,1]
nb_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>%
set_mode("classification") %>%
set_engine("naivebayes") # install discrim library for the naivebayes eng6
nb_wf <- workflow() %>%
add_recipe(nb_recipe) %>%
add_model(nb_model)
## Tune smoothness and Laplace here
tuning_grid_nb <- grid_regular(Laplace(),
smoothness(),
levels = 5) ## L^2 total tuning possibilities
## Set up K-fold CV
folds <- vfold_cv(train, v = 5, repeats=1)
CV_results_nb <- nb_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid_nb,
metrics=metric_set(accuracy))
#Random Forest
my_recipe <- recipe(sales ~., data=storeItem) %>%
step_date(date, features="doy") %>%
step_date(date, features="dow") %>%
step_naomit()
library(patchwork)
library(timetk)
library(tidyverse)
library(ggplot2)
library(vroom)
library(tidymodels)
library(embed)
library(ranger)
library(discrim)
library(naivebayes)
library(kknn)
library(themis)
train <- vroom("train.csv")
test <-  vroom("test.csv")
setwd("C:/Users/brook/Downloads/STAT348/StoreItemDemand/Store Item Demand")
train <- vroom("train.csv")
test <-  vroom("test.csv")
storeItem <- train %>%
filter(store==7, item==17)
#Random Forest
my_recipe <- recipe(sales ~., data=storeItem) %>%
step_date(date, features="doy") %>%
step_date(date, features="dow") %>%
step_naomit()
#Exponential Smoothing
library(modeltime)
install.packages('modeltime')
#Exponential Smoothing
library(modeltime)
library(timetk)
#Exponential Smoothing
library(modeltime)
library(timetk)
train <- train %>%
filter(store==7, item==17)
cv_split <- time_series_split(train, assess='3 months', cumulative = TRUE)
cv_split %>%
tk_time_series_cv_plan() %>%
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
es_model <- exp_smoothing() %>%
set_engine("ets") %>%
fit(sales-date, data=train(split))
cv_results <- modeltime_calibrate(es_model, new_data = testing(split))
cv_results <- modeltime_calibrate(es_model, new_data = test(split))
cv_results <- modeltime_calibrate(es_model, new_data = test(split))
test <-  vroom("test.csv")
cv_results <- modeltime_calibrate(es_model, new_data = test(split))
train <- train %>%
filter(store==7, item==8)
cv_split <- time_series_split(train, assess='3 months', cumulative = TRUE)
#1, 18
cv_split <- time_series_split(train, assess='3 months', cumulative = TRUE)
cv_split %>%
tk_time_series_cv_plan() %>%
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
es_model <- exp_smoothing() %>%
set_engine("ets") %>%
fit(sales-date, data=train(split))
train <- vroom("train.csv")
Item1 <- train %>%
filter(store==7, item==8)
Item1 <- train %>%
filter(store==7, item==8)
Item2 <- train %>%
filter(store==1, item==18)
cv_split <- time_series_split(Item1, assess='3 months', cumulative = TRUE)
cv_split %>%
tk_time_series_cv_plan() %>%
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
cv_split <- time_series_split(Item1, assess='3 months', cumulative = TRUE)
cv_split %>%
tk_time_series_cv_plan() %>%
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
es_model <- exp_smoothing() %>%
set_engine("ets") %>%
fit(sales~date, data=training(split))
es_model <- exp_smoothing() %>%
set_engine("ets") %>%
fit(sales~date, data=Item1(split))
es_model <- exp_smoothing() %>%
set_engine("ets") %>%
fit(sales~date, data=training(cv_split))
## Cross-validate to tune model
cv_results <- modeltime_calibrate(es_model,
new_data = testing(cv_split))
## Visualize CV results
cv_results %>%
modeltime_forecast(
new_data = testing(cv_split),
actual_data = train
) %>%
plot_modeltime_forecast(.interactive=TRUE)
## Visualize CV results
cv_results %>%
modeltime_forecast(
new_data = testing(cv_split),
actual_data = Item1
) %>%
plot_modeltime_forecast(.interactive=TRUE)
## Visualize CV results
Plot1_Nov17 <- cv_results %>%
modeltime_forecast(
new_data = testing(cv_split),
actual_data = Item1
) %>%
plot_modeltime_forecast(.interactive=TRUE)
cv_split2 <- time_series_split(Item2, assess='3 months', cumulative = TRUE)
cv_split2 %>%
tk_time_series_cv_plan() %>%
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
es_model2 <- exp_smoothing() %>%
set_engine("ets") %>%
fit(sales~date, data=training(cv_split2))
cv_results2 <- modeltime_calibrate(es_model,
new_data = testing(cv_split2))
Plot2_Nov17 <- cv_results2 %>%
modeltime_forecast(
new_data = testing(cv_split2),
actual_data = Item2
) %>%
plot_modeltime_forecast(.interactive=TRUE)
Plot2_Nov17
## Evaluate the accuracy
cv_results1 %>%
modeltime_accuracy() %>%
table_modeltime_accuracy(
.interactive = FALSE
)
## Evaluate the accuracy
cv_results %>%
modeltime_accuracy() %>%
table_modeltime_accuracy(
.interactive = FALSE
)
cv_results2 %>%
modeltime_accuracy() %>%
table_modeltime_accuracy(
.interactive = FALSE
)
es_fullfit <- cv_results %>%
modeltime_refit(data = Item1)
es_fullfit <- cv_results2 %>%
modeltime_refit(data = Item2)
es_preds <- es_fullfit %>%
modeltime_forecast(h = "3 months") %>%
rename(date=.index, sales=.value) %>%
select(date, sales) %>%
full_join(., y=test, by="date") %>%
select(id, sales)
es_fullfit <- cv_results %>%
modeltime_refit(data = Item1)
es_fullfit2 <- cv_results2 %>%
modeltime_refit(data = Item2)
es_preds <- es_fullfit %>%
modeltime_forecast(h = "3 months") %>%
rename(date=.index, sales=.value) %>%
select(date, sales) %>%
full_join(., y=test, by="date") %>%
select(id, sales)
es_preds2 <- es_fullfit2 %>%
modeltime_forecast(h = "3 months") %>%
rename(date=.index, sales=.value) %>%
select(date, sales) %>%
full_join(., y=test, by="date") %>%
select(id, sales)
es_fullfit %>%
modeltime_forecast(h = "3 months", actual_data = Item1) %>%
plot_modeltime_forecast(.interactive=FALSE)
es_fullfit2 %>%
modeltime_forecast(h = "3 months", actual_data = Item1) %>%
plot_modeltime_forecast(.interactive=FALSE)
Plot4_Nov17 <- es_fullfit2 %>%
modeltime_forecast(h = "3 months", actual_data = Item1) %>%
plot_modeltime_forecast(.interactive=FALSE)
Plot3_Nov17 <-  es_fullfit %>%
modeltime_forecast(h = "3 months", actual_data = Item1) %>%
plot_modeltime_forecast(.interactive=FALSE)
Plot3_Nov17
Plot4_Nov17
install.packages("plotly")
install.packages("plotly")
plotly::subplot(Plot1_Nov17, Plot2_Nov17, Plot3_Nov17, Plot4_Nov17)
plotly::subplot(Plot1_Nov17, Plot2_Nov17, Plot3_Nov17, Plot4_Nov17, nrows=2)
